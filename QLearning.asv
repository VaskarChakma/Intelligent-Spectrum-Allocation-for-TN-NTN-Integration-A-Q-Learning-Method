clear all; close all; clc;

%% Create Output Directory
if ~exist('results', 'dir')
    mkdir('results');
end

fprintf('TN-NTN Spectrum Allocation Q-Learning Simulation \n\n');

%  SECTION 1: SYSTEM PARAMETERS

% Network Configuration
params.n_channels = 10;              % Number of spectrum channels
params.n_TN_users = 15;              % Terrestrial network users
params.n_NTN_users = 8;              % Non-terrestrial network users
params.total_bandwidth = 100e6;      % Total bandwidth: 100 MHz
params.channel_bandwidth = params.total_bandwidth / params.n_channels;

% LEO Satellite Parameters
params.sat_altitude = 550e3;         % 550 km (LEO)
params.sat_velocity = 7.5e3;         % 7.5 km/s
params.earth_radius = 6371e3;        % Earth radius
params.coverage_angle = 45;          % degrees

% Q-Learning Parameters
params.n_episodes = 5000;            % Training episodes
params.n_steps = 100;                % Steps per episode
params.alpha = 0.1;                  % Learning rate
params.gamma = 0.95;                 % Discount factor
params.epsilon_start = 1.0;          % Initial exploration
params.epsilon_end = 0.01;           % Final exploration
params.epsilon_decay = 0.995;        % Decay rate

% Traffic Parameters
params.TN_traffic_mean = 50;         % Mbps
params.TN_traffic_std = 20;
params.NTN_traffic_mean = 30;        % Mbps
params.NTN_traffic_std = 15;

% Channel Model Parameters
params.path_loss_exp_TN = 3.5;       % Urban environment
params.path_loss_exp_NTN = 2.0;      % Free space
params.shadowing_std_TN = 8;         % dB
params.shadowing_std_NTN = 4;        % dB
params.noise_power_dBm = -100;       % Noise floor

fprintf('System configured with %d channels, %d TN users, %d NTN users\n', ...
    params.n_channels, params.n_TN_users, params.n_NTN_users);

%  SECTION 2: Q-LEARNING IMPLEMENTATION

% State space: [TN_load, NTN_load, interference_level, time_of_day]
% Discretized into bins for tabular Q-learning
n_load_bins = 10;
n_interference_bins = 5;
n_time_bins = 4;  % Morning, Afternoon, Evening, Night

n_states = n_load_bins * n_load_bins * n_interference_bins * n_time_bins;
n_actions = params.n_channels;  % Which channel to allocate

% Initialize Q-table
Q_table = zeros(n_states, n_actions);

% Training metrics
episode_rewards = zeros(params.n_episodes, 1);
episode_throughput = zeros(params.n_episodes, 1);
episode_fairness = zeros(params.n_episodes, 1);
convergence_data = zeros(params.n_episodes, 1);

epsilon = params.epsilon_start;

fprintf('Training Q-Learning agent...\n');

% Training loop
for episode = 1:params.n_episodes
    
    if mod(episode, 50) == 0
        fprintf('Episode %d/%d (epsilon=%.3f)\n', episode, params.n_episodes, epsilon);
    end
    
    total_reward = 0;
    total_throughput = 0;
    fairness_scores = [];
    
    % Initialize episode
    TN_loads = rand(1, params.n_channels) * 100;
    NTN_loads = rand(1, params.n_channels) * 100;
    
    for step = 1:params.n_steps
        
        % Simulate time of day (changes throughout episode)
        time_of_day = mod(floor(step / 25), 4) + 1;
        
        % Dynamic traffic patterns
        traffic_multiplier = 1 + 0.5 * sin(2*pi*step/params.n_steps);
        TN_demand = params.TN_traffic_mean * traffic_multiplier + ...
                    randn * params.TN_traffic_std;
        NTN_demand = params.NTN_traffic_mean * traffic_multiplier + ...
                     randn * params.NTN_traffic_std;
        
        % Calculate interference
        interference = sum(abs(TN_loads - NTN_loads)) / params.n_channels;
        
        % Discretize state
        TN_load_bin = min(ceil(mean(TN_loads) / 10), n_load_bins);
        NTN_load_bin = min(ceil(mean(NTN_loads) / 10), n_load_bins);
        interference_bin = min(ceil(interference / 20), n_interference_bins);
        
        state_idx = sub2ind([n_load_bins, n_load_bins, n_interference_bins, n_time_bins], ...
                            TN_load_bin, NTN_load_bin, interference_bin, time_of_day);
        
        % Epsilon-greedy action selection
        if rand < epsilon
            action = randi(n_actions);  % Explore
        else
            [~, action] = max(Q_table(state_idx, :));  % Exploit
        end
        
        % Simulate action effect
        old_TN_load = TN_loads(action);
        old_NTN_load = NTN_loads(action);
        
        % Allocate spectrum (simplified allocation logic)
        if TN_demand > NTN_demand
            TN_loads(action) = min(100, TN_loads(action) + TN_demand * 0.1);
            NTN_loads(action) = max(0, NTN_loads(action) - TN_demand * 0.05);
        else
            NTN_loads(action) = min(100, NTN_loads(action) + NTN_demand * 0.1);
            TN_loads(action) = max(0, TN_loads(action) - NTN_demand * 0.05);
        end
        
        % Calculate reward components
        throughput = (TN_demand * (100 - TN_loads(action))/100 + ...
                     NTN_demand * (100 - NTN_loads(action))/100);
        
        load_balance = -abs(mean(TN_loads) - mean(NTN_loads)) / 50;
        interference_penalty = -interference / 100;
        
        % Fairness (Jain's fairness index)
        all_loads = [TN_loads, NTN_loads];
        fairness = (sum(all_loads))^2 / (length(all_loads) * sum(all_loads.^2));
        
        reward = throughput + load_balance * 0.3 + interference_penalty * 0.2 + fairness * 10;
        
        % Get next state
        new_interference = sum(abs(TN_loads - NTN_loads)) / params.n_channels;
        new_TN_load_bin = min(ceil(mean(TN_loads) / 10), n_load_bins);
        new_NTN_load_bin = min(ceil(mean(NTN_loads) / 10), n_load_bins);
        new_interference_bin = min(ceil(new_interference / 20), n_interference_bins);
        
        next_state_idx = sub2ind([n_load_bins, n_load_bins, n_interference_bins, n_time_bins], ...
                                 new_TN_load_bin, new_NTN_load_bin, new_interference_bin, time_of_day);
        
        % Q-Learning update
        best_next_Q = max(Q_table(next_state_idx, :));
        Q_table(state_idx, action) = Q_table(state_idx, action) + ...
            params.alpha * (reward + params.gamma * best_next_Q - Q_table(state_idx, action));
        
        % Accumulate metrics
        total_reward = total_reward + reward;
        total_throughput = total_throughput + throughput;
        fairness_scores = [fairness_scores, fairness];
    end
    
    % Store episode metrics
    episode_rewards(episode) = total_reward / params.n_steps;
    episode_throughput(episode) = total_throughput / params.n_steps;
    episode_fairness(episode) = mean(fairness_scores);
    convergence_data(episode) = max(Q_table(:));
    
    % Decay epsilon
    epsilon = max(params.epsilon_end, epsilon * params.epsilon_decay);
end

fprintf('Training completed!\n');


%  SECTION 3: BASELINE ALGORITHM

% Random allocation baseline
random_rewards = zeros(100, 1);
random_throughput = zeros(100, 1);
for trial = 1:100
    TN_loads = rand(1, params.n_channels) * 100;
    NTN_loads = rand(1, params.n_channels) * 100;
    
    total_r = 0;
    total_t = 0;
    for step = 1:params.n_steps
        action = randi(params.n_channels);
        traffic_mult = 1 + 0.5 * sin(2*pi*step/params.n_steps);
        TN_demand = params.TN_traffic_mean * traffic_mult;
        NTN_demand = params.NTN_traffic_mean * traffic_mult;
        
        throughput = (TN_demand * (100 - TN_loads(action))/100 + ...
                     NTN_demand * (100 - NTN_loads(action))/100);
        interference = sum(abs(TN_loads - NTN_loads)) / params.n_channels;
        reward = throughput - interference/100;
        
        total_r = total_r + reward;
        total_t = total_t + throughput;
    end
    random_rewards(trial) = total_r / params.n_steps;
    random_throughput(trial) = total_t / params.n_steps;
end

fprintf('Random baseline computed.\n');


%  SECTION 4: PERFORMANCE EVALUATION

% Test trained policy
test_episodes = 50;
test_rewards = zeros(test_episodes, 1);
test_throughput = zeros(test_episodes, 1);
test_interference = zeros(test_episodes, params.n_steps);
channel_utilization = zeros(test_episodes, params.n_channels);

for episode = 1:test_episodes
    TN_loads = rand(1, params.n_channels) * 100;
    NTN_loads = rand(1, params.n_channels) * 100;
    
    total_reward = 0;
    total_throughput = 0;
    
    for step = 1:params.n_steps
        time_of_day = mod(floor(step / 25), 4) + 1;
        
        traffic_multiplier = 1 + 0.5 * sin(2*pi*step/params.n_steps);
        TN_demand = params.TN_traffic_mean * traffic_multiplier;
        NTN_demand = params.NTN_traffic_mean * traffic_multiplier;
        
        interference = sum(abs(TN_loads - NTN_loads)) / params.n_channels;
        
        TN_load_bin = min(ceil(mean(TN_loads) / 10), n_load_bins);
        NTN_load_bin = min(ceil(mean(NTN_loads) / 10), n_load_bins);
        interference_bin = min(ceil(interference / 20), n_interference_bins);
        
        state_idx = sub2ind([n_load_bins, n_load_bins, n_interference_bins, n_time_bins], ...
                            TN_load_bin, NTN_load_bin, interference_bin, time_of_day);
        
        [~, action] = max(Q_table(state_idx, :));
        
        throughput = (TN_demand * (100 - TN_loads(action))/100 + ...
                     NTN_demand * (100 - NTN_loads(action))/100);
        
        load_balance = -abs(mean(TN_loads) - mean(NTN_loads)) / 50;
        interference_penalty = -interference / 100;
        reward = throughput + load_balance * 0.3 + interference_penalty * 0.2;
        
        total_reward = total_reward + reward;
        total_throughput = total_throughput + throughput;
        test_interference(episode, step) = interference;
        
        channel_utilization(episode, action) = channel_utilization(episode, action) + 1;
    end
    
    test_rewards(episode) = total_reward / params.n_steps;
    test_throughput(episode) = total_throughput / params.n_steps;
end

fprintf('Testing completed.\n\n');


%  SECTION 5: RESULTS SUMMARY

fprintf('===== PERFORMANCE COMPARISON ===\n');
fprintf('Q-Learning Average Reward: %.2f ± %.2f\n', mean(test_rewards), std(test_rewards));
fprintf('Random Average Reward: %.2f ± %.2f\n', mean(random_rewards), std(random_rewards));
fprintf('\n');
fprintf('Q-Learning Throughput: %.2f Mbps\n', mean(test_throughput));
fprintf('Random Throughput: %.2f Mbps\n', mean(random_throughput));
fprintf('\n');
fprintf('Improvement over Random: %.1f%%\n', ...
    (mean(test_throughput) - mean(random_throughput))/mean(random_throughput)*100);
fprintf('\n');

%  SECTION 6: VISUALIZATION

% Set default figure properties for publication quality
set(0, 'DefaultAxesFontSize', 11);
set(0, 'DefaultAxesFontName', 'Times New Roman');
set(0, 'DefaultTextFontSize', 11);
set(0, 'DefaultTextFontName', 'Times New Roman');

%% FIGURE 1: Learning Curve (Convergence)
figure('Position', [100, 100, 800, 600]);
subplot(3,1,1);
plot(1:params.n_episodes, episode_rewards, 'LineWidth', 2, 'Color', [0.2 0.4 0.8]);
hold on;
% Add moving average
window = 20;
moving_avg = movmean(episode_rewards, window);
plot(1:params.n_episodes, moving_avg, 'LineWidth', 2.5, 'Color', [0.8 0.2 0.2]);
grid on;
xlabel('Training Episode');
ylabel('Average Reward');
title('(a) Learning Convergence: Average Reward per Episode');
legend('Raw Reward', sprintf('%d-Episode Moving Average', window), 'Location', 'southeast');
xlim([1 params.n_episodes]);

subplot(3,1,2);
plot(1:params.n_episodes, episode_throughput, 'LineWidth', 2, 'Color', [0.2 0.7 0.3]);
hold on;
moving_avg_tp = movmean(episode_throughput, window);
plot(1:params.n_episodes, moving_avg_tp, 'LineWidth', 2.5, 'Color', [0.7 0.3 0.2]);
grid on;
xlabel('Training Episode');
ylabel('Throughput (Mbps)');
title('(b) System Throughput Evolution During Training');
legend('Instantaneous', sprintf('%d-Episode Moving Average', window), 'Location', 'southeast');
xlim([1 params.n_episodes]);

subplot(3,1,3);
plot(1:params.n_episodes, episode_fairness, 'LineWidth', 2, 'Color', [0.7 0.2 0.7]);
hold on;
moving_avg_fair = movmean(episode_fairness, window);
plot(1:params.n_episodes, moving_avg_fair, 'LineWidth', 2.5, 'Color', [0.2 0.7 0.7]);
grid on;
xlabel('Training Episode');
ylabel('Fairness Index');
title('(c) Fairness Index (Jain''s Index) Evolution');
legend('Instantaneous', sprintf('%d-Episode Moving Average', window), 'Location', 'southeast');
xlim([1 params.n_episodes]);
ylim([0 1]);

saveas(gcf, 'results/Fig1_Learning_Convergence.png');
saveas(gcf, 'results/Fig1_Learning_Convergence.fig');
fprintf('   Figure 1 saved: Learning Convergence\n');

%% FIGURE 2: Performance Comparison (Q-Learning vs Random Only)
figure('Position', [100, 100, 900, 500]);

subplot(1,2,1);
methods = {'Q-Learning', 'Random'};
rewards_data = [mean(test_rewards), mean(random_rewards)];
errors = [std(test_rewards), std(random_rewards)];
bar_handle = bar(rewards_data, 'FaceColor', 'flat');
bar_handle.CData(1,:) = [0.2 0.4 0.8];
bar_handle.CData(2,:) = [0.8 0.2 0.2];
hold on;
errorbar(1:2, rewards_data, errors, 'k.', 'LineWidth', 1.5, 'MarkerSize', 1);
set(gca, 'XTickLabel', methods);
ylabel('Average Reward');
title('(a) Average Reward Comparison');
grid on;
ylim([0 max(rewards_data)*1.3]);

subplot(1,2,2);
throughput_data = [mean(test_throughput), mean(random_throughput)];
tp_errors = [std(test_throughput), std(random_throughput)];
bar_handle2 = bar(throughput_data, 'FaceColor', 'flat');
bar_handle2.CData(1,:) = [0.2 0.7 0.3];
bar_handle2.CData(2,:) = [0.8 0.2 0.2];
hold on;
errorbar(1:2, throughput_data, tp_errors, 'k.', 'LineWidth', 1.5, 'MarkerSize', 1);
set(gca, 'XTickLabel', methods);
ylabel('Throughput (Mbps)');
title('(b) System Throughput Comparison');
grid on;
ylim([0 max(throughput_data)*1.3]);

saveas(gcf, 'results/Fig2_Performance_Comparison.png');
saveas(gcf, 'results/Fig2_Performance_Comparison.fig');
fprintf('   Figure 2 saved: Performance Comparison\n');

%% FIGURE 3: Interference Analysis
figure('Position', [100, 100, 1000, 400]);

subplot(1,2,1);
time_steps = 1:params.n_steps;
mean_interference = mean(test_interference, 1);
std_interference = std(test_interference, 1);
fill([time_steps, fliplr(time_steps)], ...
     [mean_interference + std_interference, fliplr(mean_interference - std_interference)], ...
     [0.8 0.9 1.0], 'EdgeColor', 'none', 'FaceAlpha', 0.5);
hold on;
plot(time_steps, mean_interference, 'LineWidth', 2.5, 'Color', [0.2 0.4 0.8]);
xlabel('Time Step');
ylabel('Interference Level');
title('(a) Interference Level Over Time (Q-Learning)');
grid on;
xlim([1 params.n_steps]);

subplot(1,2,2);
histogram(mean(test_interference, 2), 20, 'FaceColor', [0.2 0.7 0.3], 'EdgeColor', 'k');
xlabel('Average Interference per Episode');
ylabel('Frequency');
title('(b) Distribution of Interference Levels');
grid on;

saveas(gcf, 'results/Fig3_Interference_Analysis.png');
saveas(gcf, 'results/Fig3_Interference_Analysis.fig');
fprintf('   Figure 3 saved: Interference Analysis\n');

%% FIGURE 4: Channel Utilization Heatmap
figure('Position', [100, 100, 900, 500]);

subplot(1,2,1);
mean_utilization = mean(channel_utilization, 1);
utilization_percent = mean_utilization / params.n_steps * 100;
bar(1:params.n_channels, utilization_percent, 'FaceColor', [0.3 0.6 0.8]);
xlabel('Channel Index');
ylabel('Utilization (%)');
title('(a) Channel Utilization Distribution');
grid on;
ylim([0 max(utilization_percent)*1.2]);

subplot(1,2,2);
imagesc(channel_utilization');
colorbar;
xlabel('Test Episode');
ylabel('Channel Index');
title('(b) Channel Selection Heatmap Across Episodes');
colormap(jet);
set(gca, 'YDir', 'normal');

saveas(gcf, 'results/Fig4_Channel_Utilization.png');
saveas(gcf, 'results/Fig4_Channel_Utilization.fig');
fprintf('   Figure 4 saved: Channel Utilization\n');

%% FIGURE 5: Q-Value Evolution
figure('Position', [100, 100, 800, 500]);

% Sample Q-values for specific states
sample_states = [100, 500, 1000, 2000, 3000];
q_evolution = zeros(length(sample_states), params.n_channels);

for i = 1:length(sample_states)
    state = min(sample_states(i), n_states);
    q_evolution(i, :) = Q_table(state, :);
end

subplot(1,2,1);
bar3(q_evolution);
xlabel('Action (Channel)');
ylabel('State Sample');
zlabel('Q-Value');
title('(a) Q-Values for Sample States');
colormap(parula);
colorbar;

subplot(1,2,2);
max_q_per_state = max(Q_table, [], 2);
histogram(max_q_per_state, 50, 'FaceColor', [0.7 0.3 0.5], 'EdgeColor', 'k');
xlabel('Maximum Q-Value');
ylabel('Number of States');
title('(b) Distribution of Maximum Q-Values');
grid on;

saveas(gcf, 'results/Fig5_QValue_Analysis.png');
saveas(gcf, 'results/Fig5_QValue_Analysis.fig');
fprintf('   Figure 5 saved: Q-Value Analysis\n');

%% FIGURE 6: Dynamic Traffic Response
figure('Position', [100, 100, 1000, 600]);

% Simulate one episode with detailed tracking
TN_loads_track = zeros(params.n_steps, params.n_channels);
NTN_loads_track = zeros(params.n_steps, params.n_channels);
actions_track = zeros(params.n_steps, 1);
demands_track = zeros(params.n_steps, 2);

TN_loads = rand(1, params.n_channels) * 50;
NTN_loads = rand(1, params.n_channels) * 50;

for step = 1:params.n_steps
    time_of_day = mod(floor(step / 25), 4) + 1;
    traffic_multiplier = 1 + 0.5 * sin(2*pi*step/params.n_steps);
    
    TN_demand = params.TN_traffic_mean * traffic_multiplier;
    NTN_demand = params.NTN_traffic_mean * traffic_multiplier;
    
    demands_track(step, :) = [TN_demand, NTN_demand];
    
    interference = sum(abs(TN_loads - NTN_loads)) / params.n_channels;
    TN_load_bin = min(ceil(mean(TN_loads) / 10), n_load_bins);
    NTN_load_bin = min(ceil(mean(NTN_loads) / 10), n_load_bins);
    interference_bin = min(ceil(interference / 20), n_interference_bins);
    
    state_idx = sub2ind([n_load_bins, n_load_bins, n_interference_bins, n_time_bins], ...
                        TN_load_bin, NTN_load_bin, interference_bin, time_of_day);
    
    [~, action] = max(Q_table(state_idx, :));
    actions_track(step) = action;
    
    % Update loads
    if TN_demand > NTN_demand
        TN_loads(action) = min(100, TN_loads(action) + TN_demand * 0.1);
        NTN_loads(action) = max(0, NTN_loads(action) - TN_demand * 0.05);
    else
        NTN_loads(action) = min(100, NTN_loads(action) + NTN_demand * 0.1);
        TN_loads(action) = max(0, TN_loads(action) - NTN_demand * 0.05);
    end
    
    TN_loads_track(step, :) = TN_loads;
    NTN_loads_track(step, :) = NTN_loads;
end

subplot(3,1,1);
plot(1:params.n_steps, demands_track(:,1), 'LineWidth', 2, 'Color', [0.2 0.4 0.8]);
hold on;
plot(1:params.n_steps, demands_track(:,2), 'LineWidth', 2, 'Color', [0.8 0.4 0.2]);
xlabel('Time Step');
ylabel('Demand (Mbps)');
title('(a) Dynamic Traffic Demand Pattern');
legend('TN Demand', 'NTN Demand', 'Location', 'best');
grid on;

subplot(3,1,2);
plot(1:params.n_steps, mean(TN_loads_track, 2), 'LineWidth', 2, 'Color', [0.2 0.7 0.3]);
hold on;
plot(1:params.n_steps, mean(NTN_loads_track, 2), 'LineWidth', 2, 'Color', [0.7 0.2 0.7]);
xlabel('Time Step');
ylabel('Average Load (%)');
title('(b) Network Load Response');
legend('TN Load', 'NTN Load', 'Location', 'best');
grid on;

subplot(3,1,3);
scatter(1:params.n_steps, actions_track, 30, actions_track, 'filled');
colorbar;
xlabel('Time Step');
ylabel('Selected Channel');
title('(c) Q-Learning Channel Selection Decisions');
ylim([0.5 params.n_channels+0.5]);
colormap(jet);
grid on;

saveas(gcf, 'results/Fig6_Traffic_Response.png');
saveas(gcf, 'results/Fig6_Traffic_Response.fig');
fprintf('   Figure 6 saved: Dynamic Traffic Response\n');


%  SECTION 7: SAVE NUMERICAL RESULTS
fprintf('6. Saving numerical results...\n');

results = struct();
results.params = params;
results.Q_table = Q_table;
results.episode_rewards = episode_rewards;
results.episode_throughput = episode_throughput;
results.test_rewards = test_rewards;
results.test_throughput = test_throughput;
results.random_rewards = random_rewards;
results.random_throughput = random_throughput;
results.channel_utilization = channel_utilization;

save('results/simulation_results.mat', 'results');

% Create summary table for paper
summary_table = table();
summary_table.Method = {'Q-Learning'; 'Random'};
summary_table.AvgReward = [mean(test_rewards); mean(random_rewards)];
summary_table.StdReward = [std(test_rewards); std(random_rewards)];
summary_table.AvgThroughput = [mean(test_throughput); mean(random_throughput)];
summary_table.StdThroughput = [std(test_throughput); std(random_throughput)];

writetable(summary_table, 'results/performance_summary.csv');
